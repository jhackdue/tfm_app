{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a2ee8e2b-27c2-4d76-ab09-012971fb0777",
   "metadata": {},
   "source": [
    "#Creating frequency distribution of words using nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', download_dir=\"d:\\\\python_entornos\\\\master\\\\nltk_data\")\n",
    "\n",
    "text=\"\"\"Achievers are not afraid of Challenges, rather they relish them, thrive in them, use them. Challenges makes is stronger.\n",
    "        Challenges makes us uncomfortable. If you get comfortable with uncomfort then you will grow. Challenge the challenge \"\"\"\n",
    "\n",
    "#Tokenize the sentences from the text corpus\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "#using CountVectorizer and removing stopwords in english language\n",
    "cv1= CountVectorizer(lowercase=True, stop_words='english')\n",
    "print(cv1)\n",
    "\n",
    "#fitting the tonized senetnecs to the countvectorizer\n",
    "text_counts=cv1.fit_transform(tokenized_text)\n",
    "print(text_counts)\n",
    "\n",
    "# printing the vocabulary and the frequency distribution pf vocabulary in tokinzed sentences\n",
    "print(cv1.vocabulary_)\n",
    "print(text_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eccf39-61b8-4541-9c3c-6bf4dd8978ae",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "CHECK TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd212375-1e0e-4480-8a6e-fe19df604d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c67c0c3-a038-486a-be4f-13b1043a0ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240304e0-dfd1-4981-b09b-4dbaf699108b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1718de-9fc0-48ac-8c75-b61ade8dd0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9d51b4-ff1e-452c-a8aa-aaabbb88510d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcecd6d-f24d-49fd-89fa-0f58f193e02a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e66c81a4-1282-4126-86fd-676a333e5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131c31ff-c55c-4f73-9ae1-48cc714614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to translation file\n",
    "path_to_data = 'spa.txt'\n",
    "\n",
    "# Read file\n",
    "translation_file = open(path_to_data,\"r\", encoding='utf-8') \n",
    "raw_data = translation_file.read()\n",
    "translation_file.close()\n",
    "\n",
    "# Parse data\n",
    "raw_data = raw_data.split('\\n')\n",
    "pairs = [sentence.split('\\t') for sentence in  raw_data]\n",
    "pairs = pairs[:-1] # skip last empty element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e130c36c-80c1-4817-85c0-e23f86e4e0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English example in pair 6:  Tom works.\n",
      "Spanish example in pair 6:  Tomás trabaja.\n",
      "English example in pair 7:  Tom'll go.\n",
      "Spanish example in pair 7:  Tom irá.\n",
      "English example in pair 8:  Tom's fat.\n",
      "Spanish example in pair 8:  Tom está gordo.\n",
      "English example in pair 9:  Tom's mad.\n",
      "Spanish example in pair 9:  Tom está loco.\n",
      "English example in pair 10:  Tom's sad.\n",
      "Spanish example in pair 10:  Tom está triste.\n"
     ]
    }
   ],
   "source": [
    "pairs2 = pairs[1000:20000]\n",
    "\n",
    "for idx_sample in range(5,10):\n",
    "    print('English example in pair {}:  {}'.format(idx_sample + 1, pairs2[idx_sample][0]))\n",
    "    print('Spanish example in pair {}:  {}'.format(idx_sample + 1, pairs2[idx_sample][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee29c0-cd77-497f-8402-5a7518526394",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874275e7-2d1f-4919-89d3-70a8678a11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # Lower case the sentence\n",
    "    lower_case_sent = sentence.lower()\n",
    "    # Strip punctuation\n",
    "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
    "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
    "   \n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57fe9607-53b5-46e5-b424-e456da855479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will surf today \n"
     ]
    }
   ],
   "source": [
    "print(clean_sentence(\"I will surf today !!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0a410f-d05f-4ca5-a136-94159231c146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: i is converted to number 1\n",
      "Word: will is converted to number 2\n",
      "Word: to is converted to number 3\n",
      "Word: the is converted to number 4\n",
      "Word: beach is converted to number 5\n",
      "Word: surf is converted to number 6\n",
      "Word: today is converted to number 7\n",
      "Word: this is converted to number 8\n",
      "Word: week is converted to number 9\n",
      "Word: travel is converted to number 10\n",
      "Word: he is converted to number 11\n",
      "Word: went is converted to number 12\n",
      "Word: his is converted to number 13\n",
      "Word: house is converted to number 14\n",
      "Word: by is converted to number 15\n"
     ]
    }
   ],
   "source": [
    "text_examples = [\n",
    "    'i will surf today',\n",
    "    'this week i will travel to the beach',\n",
    "    'he went to his house by the beach',]\n",
    "\n",
    "# Create tokenizer\n",
    "exp_text_tokenizer = Tokenizer()\n",
    "# Create word index\n",
    "exp_text_tokenizer.fit_on_texts(text_examples)\n",
    "\n",
    "for key, value in exp_text_tokenizer.word_index.items():\n",
    "    print(\"Word: {} is converted to number {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24358c0-75dc-4f26-8989-47fd1351e7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 6, 7], [8, 9, 1, 2, 10, 3, 4, 5], [11, 12, 3, 13, 14, 15, 4, 5]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_text_tokenized = exp_text_tokenizer.texts_to_sequences(text_examples)\n",
    "exp_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a9802e-2081-4ab2-9a84-ef3b5a8ad1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:  i will surf today\n",
      "Output vector: [1, 2, 6, 7] \n",
      "\n",
      "Input sentence:  this week i will travel to the beach\n",
      "Output vector: [8, 9, 1, 2, 10, 3, 4, 5] \n",
      "\n",
      "Input sentence:  he went to his house by the beach\n",
      "Output vector: [11, 12, 3, 13, 14, 15, 4, 5] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent, token_sent in zip(text_examples, exp_text_tokenized):\n",
    "    print('Input sentence:  {}'.format(sent))\n",
    "    print('Output vector: {} \\n'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e073afb-8bbf-42ab-afaa-b6bfd647bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    # Create tokenizer\n",
    "    text_tokenizer = Tokenizer()\n",
    "    # Fit texts\n",
    "    text_tokenizer.fit_on_texts(sentences)\n",
    "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "689242b8-1a83-40c8-b50d-1e644a6ce3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length spanish sentence: 68\n",
      "Maximum length english sentence: 70\n",
      "Spanish vocabulary is of 27613 unique words\n",
      "English vocabulary is of 14305 unique words\n"
     ]
    }
   ],
   "source": [
    "# Clean sentences\n",
    "english_sentences = [clean_sentence(pair[0]) for pair in pairs]\n",
    "spanish_sentences = [clean_sentence(pair[1]) for pair in pairs]\n",
    "\n",
    "# Tokenize words\n",
    "spa_text_tokenized, spa_text_tokenizer = tokenize(spanish_sentences)\n",
    "eng_text_tokenized, eng_text_tokenizer = tokenize(english_sentences)\n",
    "\n",
    "print('Maximum length spanish sentence: {}'.format(len(max(spa_text_tokenized,key=len))))\n",
    "print('Maximum length english sentence: {}'.format(len(max(eng_text_tokenized,key=len))))\n",
    "\n",
    "# Check language length\n",
    "spanish_vocab = len(spa_text_tokenizer.word_index) + 1\n",
    "english_vocab = len(eng_text_tokenizer.word_index) + 1\n",
    "print(\"Spanish vocabulary is of {} unique words\".format(spanish_vocab))\n",
    "print(\"English vocabulary is of {} unique words\".format(english_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139c217-2146-4837-aa07-cba203080b08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d138a802-6cfb-4f16-a866-0945545dd001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of example sentence: 8\n",
      "Example sentence 1:\n",
      "  -Input:[1, 2, 6, 7]\n",
      "  -Output:[1 2 6 7 0 0 0 0]\n",
      "Example sentence 2:\n",
      "  -Input:[8, 9, 1, 2, 10, 3, 4, 5]\n",
      "  -Output:[ 8  9  1  2 10  3  4  5]\n",
      "Example sentence 3:\n",
      "  -Input:[11, 12, 3, 13, 14, 15, 4, 5]\n",
      "  -Output:[11 12  3 13 14 15  4  5]\n"
     ]
    }
   ],
   "source": [
    "print('Maximum length of example sentence: {}'.format(len(max(exp_text_tokenized,key=len))))\n",
    "# Pad tokenize vectors\n",
    "exp_pad_sentence = pad_sequences(exp_text_tokenized, 8, padding = \"post\") # 8 is the max length\n",
    "for index, pad_sentence in enumerate(exp_pad_sentence):\n",
    "    print(\"Example sentence {}:\".format(index+1))\n",
    "    print(\"  -Input:{}\".format(exp_text_tokenized[index]))\n",
    "    print(\"  -Output:{}\".format(pad_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca976740-032a-4f90-bca5-4f48787c9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = max(len(max(spa_text_tokenized,key=len)), len(max(eng_text_tokenized,key=len)))\n",
    "spa_pad_sentence = pad_sequences(spa_text_tokenized, max_sentence_length, padding = \"post\")\n",
    "eng_pad_sentence = pad_sequences(eng_text_tokenized, max_sentence_length, padding = \"post\")\n",
    "\n",
    "# Reshape data\n",
    "spa_pad_sentence = spa_pad_sentence.reshape(*spa_pad_sentence.shape, 1)\n",
    "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31073269-051b-4067-8a60-6f7d61c392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (max_sentence_length, 1)\n",
    "input_sequence = Input(input_shape, name='InputLayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24a8a17e-41c1-45b5-9c9a-f639155811f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LSTM(256, return_sequences=True, dropout=0.5, name='RNNLayer')(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa24fa54-805e-46bd-817a-8c3d00e1d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = TimeDistributed(Dense(spanish_vocab), name='TimeDistributed')(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8d56808-9e66-45af-b808-c2a0d01fe234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_sequence, Activation('softmax')(logits))\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(1e-2),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50c44f76-559c-40bc-9055-12af320ff3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputLayer (InputLayer)      [(None, 70, 1)]           0         \n",
      "_________________________________________________________________\n",
      "RNNLayer (LSTM)              (None, 70, 256)           264192    \n",
      "_________________________________________________________________\n",
      "TimeDistributed (TimeDistrib (None, 70, 27613)         7096541   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 70, 27613)         0         \n",
      "=================================================================\n",
      "Total params: 7,360,733\n",
      "Trainable params: 7,360,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "373cdc51-f05a-42b2-89d2-e999122967c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4492/4492 [==============================] - 1305s 290ms/step - loss: 0.6366 - accuracy: 0.9172\n",
      "Epoch 2/10\n",
      "4492/4492 [==============================] - 1313s 292ms/step - loss: 0.6067 - accuracy: 0.9180\n",
      "Epoch 3/10\n",
      "4492/4492 [==============================] - 1322s 294ms/step - loss: 0.6018 - accuracy: 0.9182\n",
      "Epoch 4/10\n",
      "4492/4492 [==============================] - 1316s 293ms/step - loss: 0.5974 - accuracy: 0.9185\n",
      "Epoch 5/10\n",
      "4492/4492 [==============================] - 1310s 292ms/step - loss: 0.5947 - accuracy: 0.9185\n",
      "Epoch 6/10\n",
      "4492/4492 [==============================] - 1310s 292ms/step - loss: 0.5925 - accuracy: 0.9186\n",
      "Epoch 7/10\n",
      "4492/4492 [==============================] - 1312s 292ms/step - loss: 0.5916 - accuracy: 0.9186\n",
      "Epoch 8/10\n",
      "4492/4492 [==============================] - 1314s 293ms/step - loss: 0.5910 - accuracy: 0.9187\n",
      "Epoch 9/10\n",
      "4492/4492 [==============================] - 1318s 293ms/step - loss: 0.5899 - accuracy: 0.9187\n",
      "Epoch 10/10\n",
      "4492/4492 [==============================] - 1322s 294ms/step - loss: 0.5892 - accuracy: 0.9187\n"
     ]
    }
   ],
   "source": [
    "model_results = model.fit(eng_pad_sentence, spa_pad_sentence, batch_size=30, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cd4bf94-437f-44e5-aaf3-1717b77388bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python_entornos\\master\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From d:\\python_entornos\\master\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: rnn_wholeDataset\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"rnn_wholeDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8dcf27c2-913f-4a4f-9476-0b7a08f39fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_sentence(logits, tokenizer):\n",
    "    \n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<empty>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e46b9054-aab7-40ed-a149-deacb794ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The english sentence is: who\n",
      "The spanish sentence is: quién\n",
      "The predicted sentence is :\n",
      "es un <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty>\n"
     ]
    }
   ],
   "source": [
    "index = 10\n",
    "print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
    "print(\"The spanish sentence is: {}\".format(spanish_sentences[index]))\n",
    "print('The predicted sentence is :')\n",
    "print(logits_to_sentence(model.predict(eng_pad_sentence[index:index+1])[0], spa_text_tokenizer))                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77306f38-1f1b-4082-963b-30389ebcea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase a traducir es: i will try to translate this sentence never trained before i think\n",
      "valor numerico predicho\n",
      "[[[4.1163930e-09 1.1301072e-03 1.8155495e-04 ... 1.5372554e-07\n",
      "   2.3025748e-08 2.4044414e-07]\n",
      "  [9.8846783e-04 4.3910826e-03 4.8491065e-03 ... 8.2510603e-08\n",
      "   3.7283610e-07 9.3152641e-09]\n",
      "  [1.1989490e-01 5.1146280e-02 2.7203370e-02 ... 1.2704072e-06\n",
      "   8.3803587e-07 9.6696844e-08]\n",
      "  ...\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]]\n",
      "Lo traducido sería: no te <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty>\n",
      "\n",
      "\n",
      "La frase a traducir es: here is another one maybe it was trained or maybe not who knows\n",
      "valor numerico predicho\n",
      "[[[4.1163930e-09 1.1301072e-03 1.8155495e-04 ... 1.5372554e-07\n",
      "   2.3025748e-08 2.4044414e-07]\n",
      "  [9.8846783e-04 4.3910826e-03 4.8491065e-03 ... 8.2510603e-08\n",
      "   3.7283610e-07 9.3152641e-09]\n",
      "  [1.1989490e-01 5.1146280e-02 2.7203370e-02 ... 1.2704072e-06\n",
      "   8.3803587e-07 9.6696844e-08]\n",
      "  ...\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]]\n",
      "Lo traducido sería: no te <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prueba = [\"I will try to translate this sentence never trained before, I think.\",\n",
    "          \"Here is another one, maybe it was trained or maybe not, who knows?\"]\n",
    "\n",
    "prueba = [clean_sentence(s) for s in prueba]\n",
    "\n",
    "prueba_tokenizada = eng_text_tokenizer.texts_to_sequences(prueba)\n",
    "prueba_padding= pad_sequences(prueba_tokenizada, max_sentence_length, padding = \"post\")\n",
    "prueba_padding = prueba_padding.reshape(*prueba_padding.shape, 1)\n",
    "\n",
    "for i, s in enumerate(prueba):\n",
    "    print(f\"La frase a traducir es: {s}\")\n",
    "    pred = model.predict(eng_pad_sentence[i:i+1])\n",
    "    print(\"valor numerico predicho\")\n",
    "    print(pred)\n",
    "    logits = pred[0]\n",
    "    pred_sent = logits_to_sentence(logits, spa_text_tokenizer)\n",
    "    print(f\"Lo traducido sería: {pred_sent}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25e68398-89f6-47cd-8703-4c703b849939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.1163930e-09, 1.1301072e-03, 1.8155495e-04, ..., 1.5372554e-07,\n",
       "        2.3025748e-08, 2.4044414e-07],\n",
       "       [9.8846783e-04, 4.3910826e-03, 4.8491065e-03, ..., 8.2510603e-08,\n",
       "        3.7283610e-07, 9.3152641e-09],\n",
       "       [1.1989490e-01, 5.1146280e-02, 2.7203370e-02, ..., 1.2704072e-06,\n",
       "        8.3803587e-07, 9.6696844e-08],\n",
       "       ...,\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c767946f-c60e-4779-aee5-bc5070aca155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca79746-a7af-425d-aabb-918b065a522f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
