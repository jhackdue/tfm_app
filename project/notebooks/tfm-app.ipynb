{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78740f79-8531-4a62-9f4f-5adef89bc748",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trabajo de Fin de Máster: Applicación Web sobre un modelo de Question Answering en español\n",
    "\n",
    "    \n",
    "Descripción de los objetivos del proyecto.\n",
    "- Imagen de Bert\n",
    "- ¿Cómo aprenderá Bert?\n",
    "- ¿Cómo disponibilizaremos la app final?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a49645-baf1-476f-acad-c81ef296efef",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Capítulo 0: Configuración Inicial\n",
    "\n",
    "### Importación de librerias\n",
    "\n",
    "Descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e894d3-5ef1-4858-a1b4-c5264f985531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "ABS_DIR = os.path.join(os.getcwd(), \"..\")\n",
    "sys.path.append(ABS_DIR)\n",
    "\n",
    "\n",
    "import utils.read_and_write as rw\n",
    "import utils.preprocesado as pp\n",
    "import train.train_utils as tu\n",
    "import predict.predict_utils as pu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790999e-916c-4202-babf-1485458b6cae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuración del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386460b8-8ebe-470e-9da5-047575c44c10",
   "metadata": {},
   "source": [
    "Desde la siguiente URL: https://huggingface.co/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\n",
    "\n",
    "* Descargamos los ficheros de entrenamiento y evalución de SQuAD en español\n",
    "* Descargamos los ficheros de vocab y config con los que se entrenó BETO (Spanish BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241422d1-9ed0-4e17-8221-d798752988eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 19:31:07,455 tfm-app.log INFO: El fichero log tfm-app.log ha sido creado\n",
      "2021-10-31 19:31:07,459 tfm-app.log INFO: El fichero train-v2.0-es.json existe en D:\\tfm_app\\src\\project\\notebooks\\..\\data\\datasets\n",
      "2021-10-31 19:31:07,460 tfm-app.log INFO: El fichero dev-v2.0-es.json existe en D:\\tfm_app\\src\\project\\notebooks\\..\\data\\datasets\n",
      "2021-10-31 19:31:07,462 tfm-app.log INFO: El fichero vocab.txt existe en D:\\tfm_app\\src\\project\\notebooks\\..\\data\\models\n",
      "2021-10-31 19:31:07,463 tfm-app.log INFO: El fichero config.json existe en D:\\tfm_app\\src\\project\\notebooks\\..\\data\\models\n"
     ]
    }
   ],
   "source": [
    "CONFIG_DIR = os.path.join(ABS_DIR, \"config\\\\\")\n",
    "\n",
    "DATA_DIR = os.path.join(ABS_DIR, \"data\\\\\")\n",
    "LOG_DIR = os.path.join(DATA_DIR, \"logs\\\\\")\n",
    "MODELS_DIR = os.path.join(DATA_DIR, \"models\\\\\")\n",
    "DATASETS_DIR = os.path.join(DATA_DIR, \"datasets\\\\\")\n",
    "CHECKPOINTS_CALLBACK_DIR = os.path.join(DATA_DIR, \"checkpoints\\\\\")\n",
    "CACHE_DIR = os.path.join(DATA_DIR, \"cache\\\\\")\n",
    "\n",
    "logger = rw.crear_logger(\"tfm-app.log\")\n",
    "\n",
    "train_path = rw.comprobar_fichero_existe(os.path.join(DATASETS_DIR, \"train-v2.0-es.json\"), logger)\n",
    "eval_path = rw.comprobar_fichero_existe(os.path.join(DATASETS_DIR, \"dev-v2.0-es.json\"), logger)\n",
    "\n",
    "vocab_BERT_path = rw.comprobar_fichero_existe(os.path.join(MODELS_DIR, \"vocab.txt\"), logger)\n",
    "config_BERT_path = rw.comprobar_fichero_existe(os.path.join(MODELS_DIR, \"config.json\"), logger)\n",
    "\n",
    "config_file = rw.cargar_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f3e5b-eee6-4f8b-9e28-91085be66a7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualización de datos\n",
    "\n",
    "Explicación de cómo son los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5a104c-b2a3-4a0e-b0c9-395b78c6c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86818, 6)\n",
      "(9905, 6)\n"
     ]
    }
   ],
   "source": [
    "train_df = rw.json_to_dataframe(train_path)\n",
    "print(train_df.shape)\n",
    "eval_df = rw.json_to_dataframe(eval_path)\n",
    "print(eval_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877eed1-42f1-46c8-93bf-d6e5c40168db",
   "metadata": {},
   "source": [
    "* ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9734de-84c1-41b4-bd14-52b5fb997560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título:\n",
      " Budismo\n",
      "------------------------------------------------------------------------------------------\n",
      "Párrafo:\n",
      " La segunda verdad es que el origen de la dukkha puede ser conocido. En el contexto de las cuatro verdades nobles, el origen de dukkha se explica comúnmente como deseo (Pali: Tanha) condicionado por la ignorancia (Pali: Avijja). En un nivel más profundo, la causa raíz del dukkha se identifica como ignorancia (Pali: Avijja) de la verdadera naturaleza de las cosas. La tercera verdad noble es que el cese completo de la dukkha es posible, y la cuarta verdad noble identifica un camino a este cese. [Nota 7]\n",
      "------------------------------------------------------------------------------------------\n",
      "Pregunta:\n",
      " El origen del dukkha se explica como deseo condicionado por qué?\n",
      "------------------------------------------------------------------------------------------\n",
      "Posicion de respuesta:\n",
      " 201\n",
      "------------------------------------------------------------------------------------------\n",
      "Respuesta:\n",
      " ignorancia\n"
     ]
    }
   ],
   "source": [
    "id_row = 5000\n",
    "print(f\"Título:\\n {train_df.iloc[id_row, 1]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Párrafo:\\n {train_df.iloc[id_row, 2]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Pregunta:\\n {train_df.iloc[id_row, 3]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Posicion de respuesta:\\n {train_df.iloc[id_row, 4]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Respuesta:\\n {train_df.iloc[id_row, 5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352c444-4122-4fed-8f37-ef3b35b9d7ee",
   "metadata": {},
   "source": [
    "* VALIDACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1807e35-592e-4e49-9029-dcd398efab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título:\n",
      " Escuela privada\n",
      "------------------------------------------------------------------------------------------\n",
      "Párrafo:\n",
      " Las escuelas religiosas afiliadas y denominacionales forman una subcategoría de escuelas privadas. Algunas de estas escuelas enseñan educación religiosa, junto con los sujetos académicos habituales para impresionar las creencias y tradiciones de su fe particular en los estudiantes que asisten. Otros usan la denominación como una etiqueta más general para describir lo que los fundadores basaron en su creencia, manteniendo una fina distinción entre académicos y religión. Incluyen escuelas parroquiales, un término que a menudo se utiliza para denotar escuelas católicas romanas. Otros grupos religiosos representados en el sector de la educación privada K-12 incluyen protestantes, judíos, musulmanes y cristianos ortodoxos.\n",
      "------------------------------------------------------------------------------------------\n",
      "Pregunta:\n",
      " Junto con musulmanes, judíos y cristianos protestantes, ¿qué grupo religioso opera en particular escuelas privadas?\n",
      "------------------------------------------------------------------------------------------\n",
      "Posicion de respuesta:\n",
      " 563\n",
      "------------------------------------------------------------------------------------------\n",
      "Respuesta:\n",
      " católica\n"
     ]
    }
   ],
   "source": [
    "id_row = 5000\n",
    "print(f\"Título:\\n {eval_df.iloc[id_row, 1]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Párrafo:\\n {eval_df.iloc[id_row, 2]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Pregunta:\\n {eval_df.iloc[id_row, 3]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Posicion de respuesta:\\n {eval_df.iloc[id_row, 4]}\")\n",
    "print(\"---\"*30)\n",
    "print(f\"Respuesta:\\n {eval_df.iloc[id_row, 5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc633059-e92b-442a-9edb-fab1cfc65475",
   "metadata": {},
   "source": [
    "---\n",
    "## Capítulo 1: Preprocesado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24a0e7-4512-4877-92b6-337523accfdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configurando el tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1693d3cf-9530-4a00-a758-637f57af595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = pp.obtener_tokenizador(vocab=vocab_BERT_path, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438d5d85-95ad-41c1-a840-e9e72d8298e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [4, 2098, 1058, 1108, 3889, 1110, 1292, 2078, 1062, 3, 5]\n",
      "tokens: ['[CLS]', 'Esto', 'es', 'una', 'prueba', 'para', 'ver', 'cómo', 'se', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizador.encode(\"Esto es una prueba para ver cómo se tokeniza\")\n",
    "print(f'ids: {tokens.ids}')\n",
    "print(f'tokens: {tokens.tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936e775-7c28-4e07-baa8-de861014814c",
   "metadata": {},
   "source": [
    "### Modificando la configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b17e121-5de7-47e1-8e3c-4bdd51ec9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_BERT_path, 'r') as json_file:\n",
    "    config_BERT = json.load(json_file)\n",
    "config_BERT.update(config_file[\"train\"][\"preprocess\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9eec0-e814-4cb3-9acb-ae8296736262",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729f16b-6ed6-4d16-aee0-f994aea86834",
   "metadata": {},
   "source": [
    "* Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65726409-85e1-41d5-9e15-3f28c5e82e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 23:27:57,657 tfm-app.log INFO: Transformado el conjunto de entrenamiento al formato SquadExample\n",
      "2021-10-14 23:28:03,531 tfm-app.log INFO: Se ha conseguido obtener los inputs y los targets para el conjunto de entrenamiento. Se han creado  85629 puntos\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, train_dataset, train_squad_objects, train_errors = pp.transformar_datos_squad(train_df, tokenizador, config_BERT, logger=logger, name_data=\"entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f2fa5-bb9e-4630-a5f1-b648505f785f",
   "metadata": {},
   "source": [
    "* Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b16b21e-cc89-4d99-9aa6-3c5184d83e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 23:28:10,812 tfm-app.log INFO: Transformado el conjunto de validación al formato SquadExample\n",
      "2021-10-14 23:28:11,388 tfm-app.log INFO: Se ha conseguido obtener los inputs y los targets para el conjunto de validación. Se han creado  9649 puntos\n"
     ]
    }
   ],
   "source": [
    "x_eval, y_eval, eval_dataset, eval_squad_objects, eval_errors = pp.transformar_datos_squad(eval_df, tokenizador, config_BERT, logger=logger, name_data=\"validación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb7ca2-74e8-43d8-a5f9-fadf45f113cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Capítulo 2: Construcción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a0884-c8d2-4282-bc55-da34fa000ff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb4ec1d-e94b-453f-a4c2-17342465d531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0.1,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 512,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 31002,\n",
       " 'max_seq_len': 384,\n",
       " 'lr': 0.05,\n",
       " 'batch_size': 64,\n",
       " 'nb_epoch': 20}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfab7e7-c85f-4085-b274-40173ca52121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 19:31:16,381 tfm-app.log INFO: Modelo entrenado con los siguientes hiperparámetros: {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 31002, 'max_seq_len': 384, 'lr': 0.05, 'batch_size': 64, 'nb_epoch': 20}\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Modelo entrenado con los siguientes hiperparámetros: {config_BERT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833da2b-a3ef-4dfe-b183-8961041fb6dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b3bbf2d-56e1-4c1a-9e51-c5f944d78774",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = tu.generar_callbacks(x_eval, y_eval, eval_squad_objects, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc612246-3a1a-421d-bbf6-d59bbe374080",
   "metadata": {},
   "source": [
    "### Formato Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf796be-0720-4ea8-a3b7-3fb6b2d183c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_keras = pp.input_formato_keras(train_dataset, config_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25c93a-1b5d-4c4a-a70d-c28d7b7b3a35",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c2e4f7e-9e5a-409a-b801-dd2c264dae00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing TFBertForQuestionAnswering: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-10-17 14:14:52,587 tfm-app.log INFO: Se ha realizado la carga del modelo para QA a partir del modelo preentrenado dccuchile/bert-base-spanish-wwm-cased.\n",
      "2021-10-17 14:14:52,592 tfm-app.log INFO: Se ha congelado la primera capa del modelo: bert\n",
      "2021-10-17 14:14:52,605 tfm-app.log INFO: Se ha compilado el modelo con Adam(lr=0.05) como Optimizador y SparseCategoricalCrossentropy como función de pérdida.\n",
      "2021-10-17 14:14:52,606 tfm-app.log INFO: Model: \"tf_bert_for_question_answering\"\n",
      "2021-10-17 14:14:52,608 tfm-app.log INFO: _________________________________________________________________\n",
      "2021-10-17 14:14:52,608 tfm-app.log INFO: Layer (type)                 Output Shape              Param #   \n",
      "2021-10-17 14:14:52,610 tfm-app.log INFO: =================================================================\n",
      "2021-10-17 14:14:52,618 tfm-app.log INFO: bert (TFBertMainLayer)       multiple                  109850880 \n",
      "2021-10-17 14:14:52,620 tfm-app.log INFO: _________________________________________________________________\n",
      "2021-10-17 14:14:52,622 tfm-app.log INFO: qa_outputs (Dense)           multiple                  1538      \n",
      "2021-10-17 14:14:52,623 tfm-app.log INFO: =================================================================\n",
      "2021-10-17 14:14:52,634 tfm-app.log INFO: Total params: 109,852,418\n",
      "2021-10-17 14:14:52,636 tfm-app.log INFO: Trainable params: 1,538\n",
      "2021-10-17 14:14:52,637 tfm-app.log INFO: Non-trainable params: 109,850,880\n",
      "2021-10-17 14:14:52,638 tfm-app.log INFO: _________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo = tu.obtener_modelo(logger=logger, config=config_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b44cbb0b-4776-4052-a601-a0a82d02d4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_question_answering\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109850880 \n",
      "_________________________________________________________________\n",
      "qa_outputs (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,852,418\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 109,850,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315ff8a-fd61-401d-a5af-245563416958",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Capítulo 3: Entrenamiento y validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67761c84-97ee-41f0-b862-306b8e4acedd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Monitorización en Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511fb0cb-e8c5-454f-8753-881490df512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el tensorboard\n",
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8137b4-652b-4448-9030-49e4808be23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir '.\\..\\\\data\\\\logs\\\\tensorboard\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a2384-859b-4395-a09b-ff10b8b4ebc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58fff213-7148-49a9-9273-c9419cb5953a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 23:30:23,287 tfm-app.log INFO: Se realizará el entrenamiento con 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   1/1338 [..............................] - ETA: 0s - loss: 11.8042 - output_1_loss: 5.9276 - output_2_loss: 5.8766WARNING:tensorflow:From d:\\python_entornos\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.9015 - output_1_loss: 3.9903 - output_2_loss: 3.9112\n",
      "Epoch 00001: loss improved from inf to 7.90154, saving model to ..\\data\\checkpoints\\model.01-7.90.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 01:44:29,703 tfm-app.log INFO: \n",
      "epoch=1, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=1, exact match score=0.00\n",
      "1338/1338 [==============================] - 8030s 6s/step - loss: 7.9015 - output_1_loss: 3.9903 - output_2_loss: 3.9112\n",
      "Epoch 2/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8848 - output_1_loss: 3.9877 - output_2_loss: 3.8971\n",
      "Epoch 00002: loss improved from 7.90154 to 7.88476, saving model to ..\\data\\checkpoints\\model.02-7.88.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 03:58:28,133 tfm-app.log INFO: \n",
      "epoch=2, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=2, exact match score=0.00\n",
      "1338/1338 [==============================] - 8033s 6s/step - loss: 7.8848 - output_1_loss: 3.9877 - output_2_loss: 3.8971\n",
      "Epoch 3/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8930 - output_1_loss: 3.9771 - output_2_loss: 3.9160\n",
      "Epoch 00003: loss did not improve from 7.88476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 06:12:16,195 tfm-app.log INFO: \n",
      "epoch=3, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=3, exact match score=0.00\n",
      "1338/1338 [==============================] - 8022s 6s/step - loss: 7.8930 - output_1_loss: 3.9771 - output_2_loss: 3.9160\n",
      "Epoch 4/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8792 - output_1_loss: 3.9715 - output_2_loss: 3.9076\n",
      "Epoch 00004: loss improved from 7.88476 to 7.87917, saving model to ..\\data\\checkpoints\\model.04-7.88.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 08:26:06,606 tfm-app.log INFO: \n",
      "epoch=4, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=4, exact match score=0.00\n",
      "1338/1338 [==============================] - 8025s 6s/step - loss: 7.8792 - output_1_loss: 3.9715 - output_2_loss: 3.9076\n",
      "Epoch 5/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8852 - output_1_loss: 3.9794 - output_2_loss: 3.9058\n",
      "Epoch 00005: loss did not improve from 7.87917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 10:39:57,931 tfm-app.log INFO: \n",
      "epoch=5, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=5, exact match score=0.00\n",
      "1338/1338 [==============================] - 8026s 6s/step - loss: 7.8852 - output_1_loss: 3.9794 - output_2_loss: 3.9058\n",
      "Epoch 6/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8759 - output_1_loss: 3.9697 - output_2_loss: 3.9062\n",
      "Epoch 00006: loss improved from 7.87917 to 7.87586, saving model to ..\\data\\checkpoints\\model.06-7.88.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 12:53:59,360 tfm-app.log INFO: \n",
      "epoch=6, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=6, exact match score=0.00\n",
      "1338/1338 [==============================] - 8036s 6s/step - loss: 7.8759 - output_1_loss: 3.9697 - output_2_loss: 3.9062\n",
      "Epoch 7/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8915 - output_1_loss: 3.9788 - output_2_loss: 3.9128\n",
      "Epoch 00007: loss did not improve from 7.87586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 15:08:04,875 tfm-app.log INFO: \n",
      "epoch=7, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=7, exact match score=0.00\n",
      "1338/1338 [==============================] - 8040s 6s/step - loss: 7.8915 - output_1_loss: 3.9788 - output_2_loss: 3.9128\n",
      "Epoch 8/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.9044 - output_1_loss: 3.9816 - output_2_loss: 3.9228\n",
      "Epoch 00008: loss did not improve from 7.87586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 17:22:07,800 tfm-app.log INFO: \n",
      "epoch=8, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=8, exact match score=0.00\n",
      "1338/1338 [==============================] - 8037s 6s/step - loss: 7.9044 - output_1_loss: 3.9816 - output_2_loss: 3.9228\n",
      "Epoch 9/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8640 - output_1_loss: 3.9720 - output_2_loss: 3.8920\n",
      "Epoch 00009: loss improved from 7.87586 to 7.86398, saving model to ..\\data\\checkpoints\\model.09-7.86.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 19:36:12,966 tfm-app.log INFO: \n",
      "epoch=9, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=9, exact match score=0.00\n",
      "1338/1338 [==============================] - 8040s 6s/step - loss: 7.8640 - output_1_loss: 3.9720 - output_2_loss: 3.8920\n",
      "Epoch 10/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8849 - output_1_loss: 3.9742 - output_2_loss: 3.9107\n",
      "Epoch 00010: loss did not improve from 7.86398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 21:50:15,184 tfm-app.log INFO: \n",
      "epoch=10, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=10, exact match score=0.00\n",
      "1338/1338 [==============================] - 8037s 6s/step - loss: 7.8849 - output_1_loss: 3.9742 - output_2_loss: 3.9107\n",
      "Epoch 11/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8931 - output_1_loss: 3.9822 - output_2_loss: 3.9109\n",
      "Epoch 00011: loss did not improve from 7.86398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 00:04:26,239 tfm-app.log INFO: \n",
      "epoch=11, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=11, exact match score=0.00\n",
      "1338/1338 [==============================] - 8045s 6s/step - loss: 7.8931 - output_1_loss: 3.9822 - output_2_loss: 3.9109\n",
      "Epoch 12/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.8745 - output_1_loss: 3.9726 - output_2_loss: 3.9019\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.005000000074505806.\n",
      "\n",
      "Epoch 00012: loss did not improve from 7.86398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 02:18:17,994 tfm-app.log INFO: \n",
      "epoch=12, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=12, exact match score=0.00\n",
      "1338/1338 [==============================] - 8026s 6s/step - loss: 7.8745 - output_1_loss: 3.9726 - output_2_loss: 3.9019\n",
      "Epoch 13/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0420 - output_1_loss: 3.5600 - output_2_loss: 3.4820\n",
      "Epoch 00013: loss improved from 7.86398 to 7.04197, saving model to ..\\data\\checkpoints\\model.13-7.04.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 04:32:13,791 tfm-app.log INFO: \n",
      "epoch=13, exact match score=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=13, exact match score=0.01\n",
      "1338/1338 [==============================] - 8030s 6s/step - loss: 7.0420 - output_1_loss: 3.5600 - output_2_loss: 3.4820\n",
      "Epoch 14/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0091 - output_1_loss: 3.5444 - output_2_loss: 3.4647\n",
      "Epoch 00014: loss improved from 7.04197 to 7.00909, saving model to ..\\data\\checkpoints\\model.14-7.01.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 06:46:02,752 tfm-app.log INFO: \n",
      "epoch=14, exact match score=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=14, exact match score=0.01\n",
      "1338/1338 [==============================] - 8023s 6s/step - loss: 7.0091 - output_1_loss: 3.5444 - output_2_loss: 3.4647\n",
      "Epoch 15/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0101 - output_1_loss: 3.5423 - output_2_loss: 3.4678\n",
      "Epoch 00015: loss did not improve from 7.00909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 08:59:52,589 tfm-app.log INFO: \n",
      "epoch=15, exact match score=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=15, exact match score=0.01\n",
      "1338/1338 [==============================] - 8024s 6s/step - loss: 7.0101 - output_1_loss: 3.5423 - output_2_loss: 3.4678\n",
      "Epoch 16/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0116 - output_1_loss: 3.5453 - output_2_loss: 3.4663\n",
      "Epoch 00016: loss did not improve from 7.00909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 11:13:43,037 tfm-app.log INFO: \n",
      "epoch=16, exact match score=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=16, exact match score=0.01\n",
      "1338/1338 [==============================] - 8025s 6s/step - loss: 7.0116 - output_1_loss: 3.5453 - output_2_loss: 3.4663\n",
      "Epoch 17/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0064 - output_1_loss: 3.5423 - output_2_loss: 3.4642\n",
      "Epoch 00017: loss improved from 7.00909 to 7.00644, saving model to ..\\data\\checkpoints\\model.17-7.01.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 13:27:36,401 tfm-app.log INFO: \n",
      "epoch=17, exact match score=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=17, exact match score=0.01\n",
      "1338/1338 [==============================] - 8028s 6s/step - loss: 7.0064 - output_1_loss: 3.5423 - output_2_loss: 3.4642\n",
      "Epoch 18/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0067 - output_1_loss: 3.5415 - output_2_loss: 3.4651\n",
      "Epoch 00018: loss did not improve from 7.00644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 15:41:33,783 tfm-app.log INFO: \n",
      "epoch=18, exact match score=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=18, exact match score=0.01\n",
      "1338/1338 [==============================] - 8032s 6s/step - loss: 7.0067 - output_1_loss: 3.5415 - output_2_loss: 3.4651\n",
      "Epoch 19/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0027 - output_1_loss: 3.5422 - output_2_loss: 3.4605\n",
      "Epoch 00019: loss improved from 7.00644 to 7.00272, saving model to ..\\data\\checkpoints\\model.19-7.00.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 17:55:25,923 tfm-app.log INFO: \n",
      "epoch=19, exact match score=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=19, exact match score=0.01\n",
      "1338/1338 [==============================] - 8027s 6s/step - loss: 7.0027 - output_1_loss: 3.5422 - output_2_loss: 3.4605\n",
      "Epoch 20/20\n",
      "1338/1338 [==============================] - ETA: 0s - loss: 7.0103 - output_1_loss: 3.5438 - output_2_loss: 3.4664\n",
      "Epoch 00020: loss did not improve from 7.00272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 20:09:19,993 tfm-app.log INFO: \n",
      "epoch=20, exact match score=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=20, exact match score=0.00\n",
      "1338/1338 [==============================] - 8028s 6s/step - loss: 7.0103 - output_1_loss: 3.5438 - output_2_loss: 3.4664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 20:09:20,172 tfm-app.log INFO: Entrenamiento finalizado!\n",
      "2021-10-16 20:09:20,248 tfm-app.log INFO: Guardando pesos...\n",
      "2021-10-16 20:09:21,484 tfm-app.log INFO: Se han guardado los pesos en: ..\\data\\models\\qa_model_squad_v2_esp\\qa_model_squad_v2_esp.h5\n",
      "2021-10-16 20:09:21,485 tfm-app.log INFO: Guardando Modelo Json...\n",
      "2021-10-16 20:09:21,487 tfm-app.log ERROR: \n"
     ]
    }
   ],
   "source": [
    "modeloHistory = tu.entrenar_modelo(modelo, train_dataset_keras, config_BERT, callbacks, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edfadc-d340-4b4c-a40b-f85132da711f",
   "metadata": {},
   "source": [
    "## Capítulo 4: Predicción sobre el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b0007-2fd9-494a-a393-6daba0b6f289",
   "metadata": {},
   "source": [
    "### Carga del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8d3e9-b50c-4d0c-9abf-8daf60e6d13d",
   "metadata": {},
   "source": [
    "* Nuestro Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a84653e-2d0a-4684-ba6b-0b8999409b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing TFBertForQuestionAnswering: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-10-31 19:31:25,088 tfm-app.log INFO: Se ha cargado el modelo qa_model_squad_v2_esp con éxito\n"
     ]
    }
   ],
   "source": [
    "modelo = tu.cargar_modelo(logger=logger, config=config_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5cfde3-45ea-43bc-a7ce-7e22bf6635ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_question_answering\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109850880 \n",
      "_________________________________________________________________\n",
      "qa_outputs (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,852,418\n",
      "Trainable params: 109,852,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273acf1-6b9e-4c67-9356-260956724e76",
   "metadata": {},
   "source": [
    "* Modelo Remoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99492e75-c4d8-4fe4-8c2d-3a7cfae574f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing TFBertForQuestionAnswering: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-10-31 19:31:28,089 tfm-app.log INFO: Se ha cargado el modelo remote_model con éxito\n"
     ]
    }
   ],
   "source": [
    "modelo_remoto = tu.cargar_modelo(model_name=\"remote_model\", logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d62ebc03-293c-45ab-abd2-e3122afb8497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_question_answering_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109850880 \n",
      "_________________________________________________________________\n",
      "qa_outputs (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,852,418\n",
      "Trainable params: 109,852,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo_remoto.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aed765-c88c-484c-9beb-a1b172fabcbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "979e56c8-b0d8-428a-862e-f0e5ce419d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9905\n"
     ]
    }
   ],
   "source": [
    "eval_df.reset_index(drop=True, inplace=True)\n",
    "print(len(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3addf045-08e3-44a4-972f-895f861356eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto:\n",
      "El servicio comenzó el 1 de septiembre de 1993 basado en la idea del entonces director ejecutivo, Sam Chisholm y Rupert Murdoch, de convertir la estrategia de negocio de la compañía a un concepto completamente basado en la fiebre. El nuevo paquete incluye cuatro canales que antes estaban disponibles de forma gratuita, transmitiendo en los satélites de Astra, así como la introducción de nuevos canales. El servicio continuó hasta el cierre del servicio analógico de BSkyB el 27 de septiembre de 2001, debido al lanzamiento y expansión de la plataforma Sky Digital. Algunos de los canales transmitieron ya sea en el cifrado claro o suave (por lo que se requería un decodificador VideoCrypt para decodificar, sin una tarjeta de suscripción) antes de su adición al paquete Sky Multichannels. A los dos meses del lanzamiento, BSkyB ganó 400,000 nuevos suscriptores, con la mayoría tomando al menos un canal premium también, lo que ayudó a BSkyB llegar a 3,5 millones de hogares a mediados de 1994. Michael Grade criticó las operaciones frente al Select Committee on National Heritage, principalmente por la falta de programación original en muchos de los nuevos canales. \n",
      "\n",
      "Pregunta:\n",
      "¿De quién serán los satélites los nuevos canales gratuitos? \n",
      "\n",
      "Respuesta:\n",
      "satélites de Astra\n"
     ]
    }
   ],
   "source": [
    "ejemplo_id = 772\n",
    "context = eval_df.loc[ejemplo_id, \"Context\"]\n",
    "print(\"Contexto:\")\n",
    "print(context, \"\\n\")\n",
    "\n",
    "question = eval_df.loc[ejemplo_id, \"Question\"]\n",
    "print(\"Pregunta:\")\n",
    "print(question, \"\\n\")\n",
    "\n",
    "respuesta = eval_df.loc[ejemplo_id, \"Text\"]\n",
    "print(\"Respuesta:\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261dc79-137d-4575-a3b3-f9efa1a84891",
   "metadata": {},
   "source": [
    "* Nuestro Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28023782-1ac0-47e5-8480-179c6341bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_predicha = pu.predict(question, context, modelo=modelo, tokenizer=tokenizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6e45b4c-48fb-4b33-ac0c-5bf74195b29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 3,\n",
       " 'end': 96,\n",
       " 'answer': 'el 1 de septiembre de 1993 basado en la idea del entonces director ejecutivo, Sam Chisholm y Rupert Murdoch, de convertir la estrategia de negocio de la compañía a un concepto completamente basado en la fiebre. El nuevo paquete incluye cuatro canales que antes estaban disponibles de forma gratuita, transmitiendo en los satélites de Astra, así como la introducción de nuevos canales. El servicio continuó hasta el cierre del servicio analógico de BSkyB el 27 de septiembre de 2001, debido al lanzamiento y expansión de la plataforma Sky Digital. Algunos de los canales transmitieron'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_predicha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "304de59b-0dfa-4144-a138-695adbe1234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>¿DE QUIÉN SERÁN LOS SATÉLITES LOS NUEVOS CANALES GRATUITOS?</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(f'<h2>{question.upper()}</h2>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "688e7888-29a1-41c8-87ab-c54aa395d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote> El servicio comenzó <mark>el 1 de septiembre de 1993 basado en la idea del entonces director ejecutivo, Sam Chisholm y Rupert Murdoch, de convertir la estrategia de negocio de la compañía a un concepto completamente basado en la fiebre. El nuevo paquete incluye cuatro canales que antes estaban disponibles de forma gratuita, transmitiendo en los satélites de Astra, así como la introducción de nuevos canales. El servicio continuó hasta el cierre del servicio analógico de BSkyB el 27 de septiembre de 2001, debido al lanzamiento y expansión de la plataforma Sky Digital. Algunos de los canales transmitieron</mark> ya sea en el cifrado claro o suave (por lo que se requería un decodificador VideoCrypt para decodificar, sin una tarjeta de suscripción) antes de su adición al paquete Sky Multichannels. A los dos meses del lanzamiento, BSkyB ganó 400,000 nuevos suscriptores, con la mayoría tomando al menos un canal premium también, lo que ayudó a BSkyB llegar a 3,5 millones de hogares a mediados de 1994. Michael Grade criticó las operaciones frente al Select Committee on National Heritage, principalmente por la falta de programación original en muchos de los nuevos canales. </blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked_text = str(context.replace(respuesta_predicha['answer'], f\"<mark>{respuesta_predicha['answer']}</mark>\"))\n",
    "HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfe2ef-5412-41a7-8c89-10cc12d4ffd1",
   "metadata": {},
   "source": [
    "* Modelo Remoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0439f13c-ccb0-485e-b047-0347274bf4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_predicha_remoto = pu.predict(question, context, modelo=modelo_remoto, tokenizer=tokenizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66fcab66-4880-4cd2-816f-bba5fc79d52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 67, 'end': 68, 'answer': 'continuó hasta'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_predicha_remoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c36fd93-b30a-44b6-8f9e-22de3630c5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>¿DE QUIÉN SERÁN LOS SATÉLITES LOS NUEVOS CANALES GRATUITOS?</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(f'<h2>{question.upper()}</h2>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75ac8563-189e-4ad7-b1dc-383c6b61b158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote> El servicio comenzó el 1 de septiembre de 1993 basado en la idea del entonces director ejecutivo, Sam Chisholm y Rupert Murdoch, de convertir la estrategia de negocio de la compañía a un concepto completamente basado en la fiebre. El nuevo paquete incluye cuatro canales que antes estaban disponibles de forma gratuita, transmitiendo en los satélites de Astra, así como la introducción de nuevos canales. El servicio <mark>continuó hasta</mark> el cierre del servicio analógico de BSkyB el 27 de septiembre de 2001, debido al lanzamiento y expansión de la plataforma Sky Digital. Algunos de los canales transmitieron ya sea en el cifrado claro o suave (por lo que se requería un decodificador VideoCrypt para decodificar, sin una tarjeta de suscripción) antes de su adición al paquete Sky Multichannels. A los dos meses del lanzamiento, BSkyB ganó 400,000 nuevos suscriptores, con la mayoría tomando al menos un canal premium también, lo que ayudó a BSkyB llegar a 3,5 millones de hogares a mediados de 1994. Michael Grade criticó las operaciones frente al Select Committee on National Heritage, principalmente por la falta de programación original en muchos de los nuevos canales. </blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked_text = str(context.replace(respuesta_predicha_remoto['answer'], f\"<mark>{respuesta_predicha_remoto['answer']}</mark>\"))\n",
    "HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab126600-2d08-4917-971b-8b2dd146a67c",
   "metadata": {},
   "source": [
    "* Usando Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a29a64f-273a-4b93-adea-2cc0dec3b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_predicha_pipeline = pu.predict(question, context, use_pipeline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0385eb2c-f3d9-495f-8c53-a9ad3fa07308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.104228203985258e-06,\n",
       " 'start': 341,\n",
       " 'end': 359,\n",
       " 'answer': 'satélites de Astra'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_predicha_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87b8a825-3ea0-4ac9-a2bd-bcbc186d53a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>¿DE QUIÉN SERÁN LOS SATÉLITES LOS NUEVOS CANALES GRATUITOS?</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(f'<h2>{question.upper()}</h2>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16315fa4-33a1-4955-9335-b36d41358134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote> El servicio comenzó el 1 de septiembre de 1993 basado en la idea del entonces director ejecutivo, Sam Chisholm y Rupert Murdoch, de convertir la estrategia de negocio de la compañía a un concepto completamente basado en la fiebre. El nuevo paquete incluye cuatro canales que antes estaban disponibles de forma gratuita, transmitiendo en los <mark>satélites de Astra</mark>, así como la introducción de nuevos canales. El servicio continuó hasta el cierre del servicio analógico de BSkyB el 27 de septiembre de 2001, debido al lanzamiento y expansión de la plataforma Sky Digital. Algunos de los canales transmitieron ya sea en el cifrado claro o suave (por lo que se requería un decodificador VideoCrypt para decodificar, sin una tarjeta de suscripción) antes de su adición al paquete Sky Multichannels. A los dos meses del lanzamiento, BSkyB ganó 400,000 nuevos suscriptores, con la mayoría tomando al menos un canal premium también, lo que ayudó a BSkyB llegar a 3,5 millones de hogares a mediados de 1994. Michael Grade criticó las operaciones frente al Select Committee on National Heritage, principalmente por la falta de programación original en muchos de los nuevos canales. </blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked_text = str(context.replace(respuesta_predicha_pipeline['answer'], f\"<mark>{respuesta_predicha_pipeline['answer']}</mark>\"))\n",
    "HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d3e2a-1af8-4030-bc09-d8c21e85e009",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
